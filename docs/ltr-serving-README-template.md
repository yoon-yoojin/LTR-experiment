# LTR Serving - Production Ranking API

> ì‹¤ì‹œê°„ Learning to Rank ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œ

## ëª©ì°¨
- [ê°œìš”](#ê°œìš”)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
- [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
- [API ëª…ì„¸](#api-ëª…ì„¸)
- [ë°°í¬ ê°€ì´ë“œ](#ë°°í¬-ê°€ì´ë“œ)
- [ëª¨ë‹ˆí„°ë§](#ëª¨ë‹ˆí„°ë§)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ê°œë°œ ê°€ì´ë“œ](#ê°œë°œ-ê°€ì´ë“œ)

---

## ê°œìš”

### í”„ë¡œì íŠ¸ ëª©ì 

`ltr-serving`ì€ [learning_to_rank](https://github.com/your-org/learning_to_rank) ë ˆí¬ì—ì„œ ê°œë°œëœ LTR ëª¨ë¸ì„ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„œë¹™í•˜ëŠ” API ì„œë²„ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- âš¡ **ì‹¤ì‹œê°„ API**: FastAPI ê¸°ë°˜ ê³ ì„±ëŠ¥ REST API
- ğŸš€ **ë¹ ë¥¸ ì¶”ë¡ **: ONNX Runtime ê¸°ë°˜ ìµœì í™”ëœ ì¶”ë¡  (PyTorch ëŒ€ë¹„ 10ë°° ë¹ ë¦„)
- ğŸ“Š **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana í†µí•©
- ğŸ”„ **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ë°°ì¹˜ inference
- ğŸ’¾ **ìºì‹±**: Redis ê¸°ë°˜ ê²°ê³¼ ìºì‹±
- ğŸ³ **ì»¨í…Œì´ë„ˆí™”**: Docker + Kubernetes ë°°í¬ ì§€ì›
- ğŸ“ˆ **A/B í…ŒìŠ¤íŠ¸**: ë‹¤ì¤‘ ëª¨ë¸ ë²„ì „ ë™ì‹œ ì„œë¹™

### ë ˆí¬ í¬ì§€ì…”ë‹

```
learning_to_rank (Research)  â†’  ltr-serving (Production)
     â†“                                â†“
ëª¨ë¸ ê°œë°œ ë° ì‹¤í—˜                  ì‹¤ì‹œê°„ API ì„œë¹™
ì˜¤í”„ë¼ì¸ í‰ê°€                     ì˜¨ë¼ì¸ ëª¨ë‹ˆí„°ë§
best_model.pt                    model.onnx
```

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ (ê²€ìƒ‰ ì„œë¹„ìŠ¤)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /rank
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Load Balancer (K8s)            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Servers (3 replicas)      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Server 1 â”‚  â”‚ Server 2 â”‚  ...  â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚
         â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ONNX Runtime      â”‚ (ëª¨ë¸ ì¶”ë¡ )
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Redis Cache       â”‚ (ê²°ê³¼ ìºì‹±)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Prometheus        â”‚ (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Grafana           â”‚ (ì‹œê°í™”)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

```
ltr-serving/
â”‚
â”œâ”€â”€ app/                    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
â”‚   â”œâ”€â”€ main.py            # FastAPI ì„œë²„
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ rank.py        # ë­í‚¹ API
â”‚   â”‚   â””â”€â”€ health.py      # Health check
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ inference.py   # ONNX ì¶”ë¡  ì„œë¹„ìŠ¤
â”‚   â”‚   â”œâ”€â”€ cache.py       # Redis ìºì‹±
â”‚   â”‚   â””â”€â”€ preprocess.py  # ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py     # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ config.py      # ì„¤ì •
â”‚       â””â”€â”€ monitoring.py  # ë©”íŠ¸ë¦­
â”‚
â”œâ”€â”€ models/                 # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸
â”‚   â”œâ”€â”€ current/
â”‚   â”‚   â”œâ”€â”€ model.onnx
â”‚   â”‚   â”œâ”€â”€ preprocessor.pkl
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ versions/
â”‚       â”œâ”€â”€ v1.0.0/
â”‚       â””â”€â”€ v1.1.0/
â”‚
â”œâ”€â”€ deployment/             # ë°°í¬ ì„¤ì •
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ ingress.yaml
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus.yml
â”‚       â””â”€â”€ grafana-dashboard.json
â”‚
â””â”€â”€ tests/                  # í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ test_api.py
    â”œâ”€â”€ test_inference.py
    â””â”€â”€ load_test.py
```

---

## ê¸°ìˆ  ìŠ¤íƒ

### Core Technologies

| ê³„ì¸µ | ê¸°ìˆ  | ìš©ë„ | ë²„ì „ |
|------|------|------|------|
| **API Framework** | FastAPI | REST API ì„œë²„ | 0.104+ |
| **Inference** | ONNX Runtime | ëª¨ë¸ ì¶”ë¡  | 1.16+ |
| **Caching** | Redis | ê²°ê³¼ ìºì‹± | 7.0+ |
| **Monitoring** | Prometheus | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | 2.45+ |
| **Visualization** | Grafana | ëŒ€ì‹œë³´ë“œ | 10.0+ |
| **Container** | Docker | ì»¨í…Œì´ë„ˆí™” | 24.0+ |
| **Orchestration** | Kubernetes | ë°°í¬ ë° ê´€ë¦¬ | 1.28+ |

### Performance Metrics

| ë©”íŠ¸ë¦­ | ëª©í‘œ | í˜„ì¬ |
|--------|------|------|
| **Latency (P50)** | < 50ms | 45ms |
| **Latency (P99)** | < 100ms | 95ms |
| **Throughput** | > 1000 QPS | 1200 QPS |
| **Error Rate** | < 0.1% | 0.05% |
| **Availability** | > 99.9% | 99.95% |

---

## ë¹ ë¥¸ ì‹œì‘

### Prerequisites

```bash
# Required
- Docker 24.0+
- Python 3.9+
- Redis (optional, for caching)

# Recommended
- Kubernetes cluster (for production)
- GPU (optional, for faster inference)
```

### Local Development

```bash
# 1. ë ˆí¬ í´ë¡ 
git clone https://github.com/your-org/ltr-serving.git
cd ltr-serving

# 2. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ ìˆ˜ì •

# 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (learning_to_rankì—ì„œ)
mkdir -p models/current
cp ../learning_to_rank/production/model.onnx models/current/
cp ../learning_to_rank/production/preprocessor.pkl models/current/

# 5. ê°œë°œ ì„œë²„ ì‹¤í–‰
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# 6. API í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/rank" \
  -H "Content-Type: application/json" \
  -d @tests/sample_request.json
```

### Dockerë¡œ ì‹¤í–‰

```bash
# 1. ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t ltr-serving:latest .

# 2. ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  ltr-serving:latest

# 3. Docker Composeë¡œ ì „ì²´ ìŠ¤íƒ ì‹¤í–‰
docker-compose up -d
```

---

## API ëª…ì„¸

### 1. Rank API

**Endpoint**: `POST /api/v1/rank`

**Request**:
```json
{
  "query_id": "q12345",
  "documents": [
    {
      "doc_id": "doc1",
      "features": [0.5, 0.8, 0.3, ...]
    },
    {
      "doc_id": "doc2",
      "features": [0.2, 0.5, 0.7, ...]
    }
  ],
  "top_k": 10
}
```

**Response**:
```json
{
  "query_id": "q12345",
  "rankings": [
    {
      "doc_id": "doc2",
      "score": 0.892,
      "rank": 1
    },
    {
      "doc_id": "doc1",
      "score": 0.745,
      "rank": 2
    }
  ],
  "latency_ms": 45,
  "model_version": "v1.0.0"
}
```

### 2. Batch Rank API

**Endpoint**: `POST /api/v1/batch_rank`

**Request**:
```json
{
  "queries": [
    {
      "query_id": "q1",
      "documents": [...]
    },
    {
      "query_id": "q2",
      "documents": [...]
    }
  ]
}
```

**Response**:
```json
{
  "results": [
    {
      "query_id": "q1",
      "rankings": [...]
    },
    {
      "query_id": "q2",
      "rankings": [...]
    }
  ],
  "total_queries": 2,
  "avg_latency_ms": 38
}
```

### 3. Health Check

**Endpoint**: `GET /health`

**Response**:
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_version": "v1.0.0",
  "uptime_seconds": 86400
}
```

### 4. Metrics

**Endpoint**: `GET /metrics`

**Response**: Prometheus í¬ë§·
```
# HELP ltr_requests_total Total number of ranking requests
# TYPE ltr_requests_total counter
ltr_requests_total{model_version="v1.0.0"} 12450

# HELP ltr_latency_seconds Latency of ranking requests
# TYPE ltr_latency_seconds histogram
ltr_latency_seconds_bucket{le="0.05"} 8234
ltr_latency_seconds_bucket{le="0.1"} 11982
```

---

## ë°°í¬ ê°€ì´ë“œ

### Kubernetes ë°°í¬

```bash
# 1. Namespace ìƒì„±
kubectl create namespace ltr-serving

# 2. ConfigMap ìƒì„±
kubectl create configmap ltr-config \
  --from-file=config.yaml \
  -n ltr-serving

# 3. Secret ìƒì„± (Redis ë¹„ë°€ë²ˆí˜¸ ë“±)
kubectl create secret generic ltr-secrets \
  --from-literal=redis-password=<password> \
  -n ltr-serving

# 4. ë°°í¬
kubectl apply -f deployment/kubernetes/

# 5. ì„œë¹„ìŠ¤ í™•ì¸
kubectl get pods -n ltr-serving
kubectl get svc -n ltr-serving

# 6. ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/ltr-serving -n ltr-serving
```

### Horizontal Pod Autoscaling

```yaml
# deployment/kubernetes/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ltr-serving-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ltr-serving
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: ltr_requests_per_second
      target:
        type: AverageValue
        averageValue: "500"
```

### ëª¨ë¸ ì—…ë°ì´íŠ¸ (Blue-Green Deployment)

```bash
# 1. ìƒˆ ëª¨ë¸ ë²„ì „ ë°°í¬
kubectl apply -f deployment/kubernetes/deployment-v1.1.0.yaml

# 2. íŠ¸ë˜í”½ ì¼ë¶€ ì „í™˜ (Canary)
kubectl patch service ltr-serving \
  -p '{"spec":{"selector":{"version":"v1.1.0"}}}'

# 3. ëª¨ë‹ˆí„°ë§ (ì—ëŸ¬ìœ¨, ì§€ì—°ì‹œê°„ í™•ì¸)

# 4. ë¬¸ì œ ì—†ìœ¼ë©´ ì „ì²´ ì „í™˜
kubectl scale deployment ltr-serving-v1.0.0 --replicas=0

# 5. ë¡¤ë°± í•„ìš”ì‹œ
kubectl scale deployment ltr-serving-v1.0.0 --replicas=3
kubectl patch service ltr-serving \
  -p '{"spec":{"selector":{"version":"v1.0.0"}}}'
```

---

## ëª¨ë‹ˆí„°ë§

### Prometheus Metrics

ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë˜ëŠ” ë©”íŠ¸ë¦­:

```python
# ìš”ì²­ ì¹´ìš´í„°
ltr_requests_total{model_version, status}

# ì§€ì—°ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
ltr_latency_seconds{model_version}

# ì—ëŸ¬ìœ¨
ltr_errors_total{error_type}

# ì²˜ë¦¬ëŸ‰
ltr_throughput_qps

# ëª¨ë¸ ë¡œë“œ ì‹œê°„
ltr_model_load_seconds
```

### Grafana ëŒ€ì‹œë³´ë“œ

**ì£¼ìš” íŒ¨ë„**:

1. **Request Rate** (QPS)
   - ì‹œê°„ë³„ ìš”ì²­ ìˆ˜
   - 5ë¶„, 1ì‹œê°„, 24ì‹œê°„ í‰ê· 

2. **Latency Distribution**
   - P50, P90, P95, P99
   - Heatmap ì‹œê°í™”

3. **Error Rate**
   - ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ë¥˜
   - ì•ŒëŒ ì„ê³„ê°’ í‘œì‹œ

4. **Model Performance**
   - ëª¨ë¸ë³„ ì²˜ë¦¬ëŸ‰
   - ë²„ì „ë³„ ë¹„êµ

5. **Resource Usage**
   - CPU, Memory ì‚¬ìš©ëŸ‰
   - Pod ìƒíƒœ

### ì•ŒëŒ ì„¤ì •

```yaml
# deployment/monitoring/alerts.yaml
groups:
- name: ltr_serving
  interval: 30s
  rules:
  - alert: HighLatency
    expr: ltr_latency_seconds{quantile="0.99"} > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "P99 latency > 100ms"

  - alert: HighErrorRate
    expr: rate(ltr_errors_total[5m]) > 0.01
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "Error rate > 1%"

  - alert: LowThroughput
    expr: rate(ltr_requests_total[5m]) < 100
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Throughput < 100 QPS"
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ONNX ìµœì í™”

```python
# ëª¨ë¸ ë³€í™˜ ì‹œ ìµœì í™”
import onnx
from onnxruntime.transformers import optimizer

# ê·¸ë˜í”„ ìµœì í™”
optimized_model = optimizer.optimize_model(
    'model.onnx',
    model_type='bert',  # ë˜ëŠ” 'gpt2'
    num_heads=8,
    hidden_size=256
)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
# app/services/inference.py
class InferenceService:
    def __init__(self, batch_size=64):
        self.batch_size = batch_size
        self.queue = []

    async def predict_batch(self, features_list):
        """ì§„ì§œ ë°°ì¹˜ inference"""
        # Padding
        max_docs = max(len(f) for f in features_list)
        batched = np.zeros((len(features_list), max_docs, num_features))

        for i, features in enumerate(features_list):
            batched[i, :len(features)] = features

        # ONNX ì¶”ë¡  (í•œ ë²ˆì—)
        scores = self.session.run(None, {'input': batched})

        return scores
```

### 3. Redis ìºì‹±

```python
# app/services/cache.py
class CacheService:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.ttl = 3600  # 1ì‹œê°„

    async def get_or_compute(self, cache_key, compute_fn):
        # ìºì‹œ ì¡°íšŒ
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # ê³„ì‚°
        result = await compute_fn()

        # ìºì‹œ ì €ì¥
        await self.redis.setex(
            cache_key,
            self.ttl,
            json.dumps(result)
        )

        return result
```

### 4. ë™ì‹œì„± ìµœì í™”

```python
# app/main.py
from fastapi import FastAPI
from fastapi.concurrency import run_in_threadpool

app = FastAPI()

@app.post("/rank")
async def rank(request: RankRequest):
    # I/O bound ì‘ì—…ì€ async
    # CPU bound ì‘ì—…ì€ threadpool
    result = await run_in_threadpool(
        inference_service.predict,
        request.features
    )
    return result
```

---

## ê°œë°œ ê°€ì´ë“œ

### ë¡œì»¬ ê°œë°œ í™˜ê²½

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements-dev.txt

# 3. Pre-commit í›… ì„¤ì •
pre-commit install

# 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/

# 5. ì½”ë“œ í¬ë§·íŒ…
black app/
isort app/

# 6. íƒ€ì… ì²´í¬
mypy app/
```

### í…ŒìŠ¤íŠ¸

```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_rank_api():
    response = client.post(
        "/api/v1/rank",
        json={
            "query_id": "test",
            "documents": [
                {"doc_id": "1", "features": [0.5] * 136}
            ]
        }
    )
    assert response.status_code == 200
    assert "rankings" in response.json()

def test_health_check():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

### ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# Locustë¥¼ ì‚¬ìš©í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
locust -f tests/load_test.py --host http://localhost:8000

# ë˜ëŠ” ê°„ë‹¨íˆ
ab -n 10000 -c 100 -p sample_request.json \
   -T application/json \
   http://localhost:8000/api/v1/rank
```

---

## ì—°ê´€ ë ˆí¬ì§€í† ë¦¬

- [learning_to_rank](https://github.com/your-org/learning_to_rank) - ëª¨ë¸ ê°œë°œ ë° ì‹¤í—˜
- [ml-platform](https://github.com/your-org/ml-platform) - MLOps ì¸í”„ë¼

---

## ë¼ì´ì„ ìŠ¤

MIT License

## ê¸°ì—¬

Pull Requestë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-04
**ë©”ì¸í…Œì´ë„ˆ**: ML Platform Team
**ë²„ì „**: 1.0.0
